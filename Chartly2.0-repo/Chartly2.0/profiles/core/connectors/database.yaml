version: "1.0"

connector_profile:
  id: "database"
  protocol: "database"
  kind: "db"
  description: >
    Database connector profile (read-only). Defines how connector-hub reads from relational databases
    using a DSN supplied via environment variable, executes safe SELECT queries, supports incremental
    ingestion, and writes raw payloads for downstream normalization.
  version: "1.0.0"
  status: "active"

source_config_schema:
  required_fields:
    - "dsn_env"
    - "query"
  optional_fields:
    - "engine"
    - "parameters"
    - "timeout_seconds"
    - "batch_size"
    - "incremental"
    - "rate_limit_rpm"
    - "max_rows_per_job"
  field_docs:
    dsn_env:
      type: "string"
      rules:
        - "Must be the name of an environment variable containing the DSN/connection string."
        - "No DSN literals allowed in source config."
    query:
      type: "string"
      rules:
        - "Must be read-only (SELECT only)."
        - "Must not contain semicolons for multi-statement execution."
        - "Must not contain: INSERT, UPDATE, DELETE, DROP, ALTER, TRUNCATE, CREATE, GRANT, REVOKE."
    engine:
      type: "string"
      rules:
        - "Optional hint; used to select driver behavior. If omitted, inferred from DSN by implementation."
    parameters:
      type: "object"
      rules:
        - "Deterministic parameters only."
        - "No secrets."
    incremental:
      type: "object"
      rules:
        - "If enabled, must specify mode and cursor field."
    batch_size:
      type: "integer"
      default: 10000
      rules:
        - "Between 1 and 100000."
    max_rows_per_job:
      type: "integer"
      default: 500000
      rules:
        - "Caps total rows per job to bound runtime and payload size."

database:
  supported_engines:
    - "postgres"
    - "mysql"
    - "sqlserver"
    - "sqlite"
  dsn_env_policy:
    required: true
    notes:
      - "DSNs are secrets or secret-adjacent. They must be provided via secret manager or env vars."
  query_policy:
    read_only_enforced: true
    forbid_patterns:
      - "(?i)\\bINSERT\\b"
      - "(?i)\\bUPDATE\\b"
      - "(?i)\\bDELETE\\b"
      - "(?i)\\bDROP\\b"
      - "(?i)\\bALTER\\b"
      - "(?i)\\bTRUNCATE\\b"
      - "(?i)\\bCREATE\\b"
      - "(?i)\\bGRANT\\b"
      - "(?i)\\bREVOKE\\b"
      - ";"
    notes:
      - "This is a defensive heuristic. Implementation should enforce server-side read-only roles as well."
  parameters_policy:
    allow: true
    deterministic_only: true
  timeout_policy:
    default_seconds: 60
    min_seconds: 1
    max_seconds: 300

incremental_ingestion:
  supported_modes: ["none", "watermark", "cursor"]
  default:
    enabled: false
    mode: "none"
  watermark:
    description: "Use a monotonic timestamp or numeric field as watermark."
    required_fields: ["field", "initial_value"]
    state_storage:
      status: "roadmap"
      notes:
        - "Watermark state stored in relational metadata (storage service) keyed by tenant_id+source_id."
  cursor:
    description: "Use a primary key cursor for incremental paging."
    required_fields: ["field", "initial_value"]
    state_storage:
      status: "roadmap"
  safety_limits:
    max_lookback_seconds: 86400
    notes:
      - "Lookback windows can prevent missed rows but increase duplicates; rely on idempotency downstream."

rate_limiting_and_backpressure:
  defaults:
    rate_limit_rpm: 30
    per_source_concurrency: 2
  backpressure:
    - "Respect orchestrator dispatch throttles."
    - "Stop reading when downstream queue depth exceeds threshold."

retries_and_circuit_breakers:
  retry_policy:
    retry_on_transient_errors: true
    max_attempts: 3
    base_backoff_ms: 1000
    max_backoff_ms: 15000
    jitter: true
  circuit_breaker:
    enabled: true
    failure_threshold: 5
    open_seconds: 120

payload_handling:
  row_to_json_policy:
    format: "array_of_objects"
    include_column_names: true
    null_handling: "preserve"
  batch_size_default: 10000
  max_rows_per_batch: 100000
  max_rows_per_job: 500000
  max_bytes_per_job: 50000000
  raw_ref_requirements:
    - "Write each batch as a raw blob (or a single blob if small enough)."
    - "Compute sha256 for each blob and emit raw_ref(s) for downstream processing."
  notes:
    - "If results exceed max_bytes_per_job, stop and emit ingestion health warn/error."

observability:
  required_metrics:
    - "db.query_total"
    - "db.rows_read_total"
    - "db.latency_ms_p95"
    - "db.errors_total"
  required_log_fields:
    - "ts"
    - "service=connector-hub"
    - "tenant_id"
    - "source_id"
    - "job_id"
    - "engine"
    - "rows_read"
    - "latency_ms"
    - "raw_sha256"

llm_assist:
  enabled: true
  allowed_roles: ["schema_drift_explainer"]
  forbidden:
    - "generate_or_modify_sql_without_human_review"
    - "suggest_destructive_sql"
    - "include_dsn_literals"
  approval_gate: "human_required"
  audit_fields_required:
    - "llm_role"
    - "model_id"
    - "prompt_hash"
    - "output_hash"
    - "approver"
    - "decision"
  notes:
    - "LLM may help interpret schema drift and propose safer SELECT queries, but never auto-applies."

# -----------------------------------------------------------------------------
# EXAMPLES (comments only)
#
# Example 1: Postgres read-only query with watermark
# source:
#   kind: db
#   connector: database
#   config:
#     engine: "postgres"
#     dsn_env: "SALES_DB_DSN"
#     query: "SELECT id, created_at, total FROM orders WHERE created_at > :watermark ORDER BY created_at ASC LIMIT :limit"
#     parameters:
#       limit: 10000
#     incremental:
#       enabled: true
#       mode: "watermark"
#       field: "created_at"
#       initial_value: "2026-01-01T00:00:00Z"
# -----------------------------------------------------------------------------
