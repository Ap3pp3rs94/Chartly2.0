version: "1.0"

mapping_dsl:
  language: "chartly-mapping-v1"
  determinism:
    required: true
    rules:
      - "All mappings MUST be deterministic: same input + same profile version + same job context => same outputs."
      - "No network calls, no randomness, no time-now, no external state reads inside mapping evaluation."
      - "Allowed time sources are job context fields (captured_at, fetched_at, requested_at)."
      - "All optional behavior must be explicitly gated by flags in profile.yaml or domain overlays."
  do_not:
    - "perform_http_requests"
    - "read_local_files"
    - "use_random_generators"
    - "use_system_clock_directly"
    - "emit_secrets_or_sensitive_payloads"
  expression_grammar:
    # Minimal documented grammar for deterministic extract/transform steps.
    # Implementations may vary but must conform to this behavior contract.
    path_syntax: "dot-path + bracket indexing (jsonpath-like subset)"
    operators:
      - "get(path, default)"
      - "coalesce(a, b, c...)"
      - "eq(a, b)"
      - "ne(a, b)"
      - "in(value, [a,b,c])"
      - "and(a,b)"
      - "or(a,b)"
      - "not(a)"
      - "len(x)"
      - "to_string(x)"
      - "to_number(x)"
      - "to_bool(x)"
      - "parse_datetime(x)"
      - "trim(x)"
      - "lower(x)"
      - "upper(x)"
      - "replace(x, from, to)"
      - "substr(x, start, length)"
      - "clamp(num, min, max)"
      - "round(num, decimals)"
      - "hash_sha256(x)"
    notes:
      - "All operations must be pure functions."
      - "If an operation fails (e.g., parse_datetime), the mapping must either emit a null (if allowed) or quarantine via rules.yaml."

input_shapes:
  supported_payload_types:
    - "object"     # JSON object
    - "array"      # JSON array
    - "text"       # stored as preview only; mapping generally emits minimal events/metrics
  normalization_of_raw_input:
    # Job context binds the raw payload and metadata to the mapping engine.
    # The raw_ref is not extracted from payload; it is bound from the ingestion pipeline.
    job_context:
      required_fields:
        - tenant_id
        - source_id
        - job_id
        - captured_at
        - raw_ref.bucket
        - raw_ref.key
        - raw_ref.sha256
        - raw_ref.content_type
        - raw_ref.captured_at
    raw_payload_location:
      # The normalizer is expected to load the raw blob and supply:
      # - payload: the upstream JSON (object/array)
      # - meta: connector metadata (optional)
      # - fetched_at: timestamp from connector (optional)
      payload_field: "payload"
      meta_field: "meta"
      fetched_at_field: "fetched_at"

canonical_outputs:
  raw_ref_binding:
    rule: "Bind raw_ref from job context to EVERY Event and Metric. Never synthesize raw_ref from payload."
  entity_defaults:
    # Base profile does not infer entities unless domain overlay enables it.
    enabled: false
    default_attributes: {}
  event_defaults:
    required_fields:
      tenant_id: "ctx.tenant_id"
      source_id: "ctx.source_id"
      raw_ref: "ctx.raw_ref"
    event_type_naming:
      convention: "dot.separated.namespaces"
      examples:
        - "ingest.scheduled"
        - "ingest.fetched"
        - "normalize.validated"
        - "normalize.quarantined"
  metric_defaults:
    required_fields:
      tenant_id: "ctx.tenant_id"
      source_id: "ctx.source_id"
      raw_ref: "ctx.raw_ref"
    metric_name_naming:
      convention: "dot.separated.namespaces"
      examples:
        - "pipeline.records_in"
        - "pipeline.records_out"
        - "pipeline.quarantine_count"

extractors:
  path_syntax:
    examples:
      - "payload.items[0].price"
      - "payload.data.orders[3].total"
      - "meta.http.status_code"
    missing_path_behavior: "return default if provided; otherwise null"
  safe_casts:
    string:
      behavior: "to_string(x) always returns a string; null becomes empty only if explicitly coalesced"
    number:
      behavior: "to_number(x) may fail; failure handling is governed by rules.yaml"
    bool:
      behavior: "to_bool(x) accepts true/false, 'true'/'false', 1/0; otherwise fails"
  timestamp_parsing:
    accepted_inputs:
      - "RFC3339"
      - "ISO8601"
      - "unix_seconds"
      - "unix_millis"
      - "common datetime strings (domain overlays may add)"
    output: "RFC3339 UTC"
    failure_handling: "rules.yaml decides: quarantine or fallback to ctx.captured_at"

transforms:
  built_in:
    - "trim"
    - "lower"
    - "upper"
    - "replace"
    - "substr"
    - "parse_int"
    - "parse_float"
    - "parse_bool"
    - "parse_datetime"
    - "to_iso8601"
    - "hash_sha256"
    - "clamp"
    - "round"
    - "redact"   # deterministic redaction (masking) ONLY; no secrets allowed in output
  constraints:
    - "No transforms may introduce non-determinism."
    - "No transforms may call network resources."
    - "No transforms may expand raw payload into logs or canonical records beyond required mapped fields."
    - "Dimension limits (max count/length) are enforced by cleansing.yaml/rules.yaml; mappings should keep dims minimal."

mapping_rules:
  base_event_emission:
    enabled: true
    description: "Emit lifecycle events for ingestion/normalization stages to support auditability and monitoring."
    events:
      - name: "ingest.scheduled"
        when: "ctx.stage == 'scheduled'"
        event:
          id: "ctx.event_id"           # generated by pipeline, not by mapping
          event_type: "ingest.scheduled"
          ts: "ctx.captured_at"
          entity_id: null
          payload:
            job_id: "ctx.job_id"
            note: "Ingest scheduled"
          tenant_id: "ctx.tenant_id"
          source_id: "ctx.source_id"
          raw_ref: "ctx.raw_ref"

      - name: "ingest.fetched"
        when: "ctx.stage == 'fetched'"
        event:
          id: "ctx.event_id"
          event_type: "ingest.fetched"
          ts: "coalesce(ctx.fetched_at, ctx.captured_at)"
          entity_id: null
          payload:
            job_id: "ctx.job_id"
            content_type: "ctx.raw_ref.content_type"
          tenant_id: "ctx.tenant_id"
          source_id: "ctx.source_id"
          raw_ref: "ctx.raw_ref"

      - name: "normalize.validated"
        when: "ctx.stage == 'validated'"
        event:
          id: "ctx.event_id"
          event_type: "normalize.validated"
          ts: "ctx.captured_at"
          entity_id: null
          payload:
            job_id: "ctx.job_id"
            profile: "core/base"
            profile_version: "1.0.0"
          tenant_id: "ctx.tenant_id"
          source_id: "ctx.source_id"
          raw_ref: "ctx.raw_ref"

      - name: "normalize.quarantined"
        when: "ctx.stage == 'quarantined'"
        event:
          id: "ctx.event_id"
          event_type: "normalize.quarantined"
          ts: "ctx.captured_at"
          entity_id: null
          payload:
            job_id: "ctx.job_id"
            reason_code: "ctx.reason_code"
          tenant_id: "ctx.tenant_id"
          source_id: "ctx.source_id"
          raw_ref: "ctx.raw_ref"

  base_metric_emission:
    enabled: true
    description: "Emit pipeline-derived operational metrics (not domain metrics) based on processing results."
    metrics:
      - name: "pipeline.records_in"
        when: "ctx.records_in != null"
        metric:
          id: "ctx.metric_id"
          metric_name: "pipeline.records_in"
          ts: "ctx.captured_at"
          value: "to_number(ctx.records_in)"
          dimensions:
            stage: "ctx.stage"
            profile: "core/base"
            profile_version: "1.0.0"
          tenant_id: "ctx.tenant_id"
          source_id: "ctx.source_id"
          raw_ref: "ctx.raw_ref"

      - name: "pipeline.records_out"
        when: "ctx.records_out != null"
        metric:
          id: "ctx.metric_id"
          metric_name: "pipeline.records_out"
          ts: "ctx.captured_at"
          value: "to_number(ctx.records_out)"
          dimensions:
            stage: "ctx.stage"
            profile: "core/base"
            profile_version: "1.0.0"
          tenant_id: "ctx.tenant_id"
          source_id: "ctx.source_id"
          raw_ref: "ctx.raw_ref"

      - name: "pipeline.quarantine_count"
        when: "ctx.quarantine_count != null"
        metric:
          id: "ctx.metric_id"
          metric_name: "pipeline.quarantine_count"
          ts: "ctx.captured_at"
          value: "to_number(ctx.quarantine_count)"
          dimensions:
            stage: "ctx.stage"
            profile: "core/base"
            profile_version: "1.0.0"
          tenant_id: "ctx.tenant_id"
          source_id: "ctx.source_id"
          raw_ref: "ctx.raw_ref"

  generic_payload_to_metrics:
    enabled: true
    description: >
      Deterministic generic mapping pattern for JSON payloads when no domain-specific mapping exists.
      This rule is intentionally conservative: it extracts numeric leaf fields from a bounded subset of the payload.
      Domain overlays should replace this with explicit mappings.
    limits:
      max_items_from_array: 50
      max_depth: 3
      max_numeric_fields_per_item: 50
    behavior:
      - "If payload is an array of objects, treat each object as an observation candidate."
      - "For each observation, emit metrics for numeric fields only."
      - "Timestamp selection order: observation.ts keys -> ctx.fetched_at -> ctx.captured_at."
      - "Dimensions include small, low-cardinality string/bool fields; enforcement/truncation occurs in cleansing/rules."
    ts_keys_priority: ["ts", "timestamp", "time", "date", "datetime", "created_at", "updated_at"]
    mapping:
      # This is a behavioral contract, not executable pseudocode.
      # Implementation must follow these rules exactly once implemented.
      numeric_field_to_metric_name:
        rule: "metric_name = 'raw.' + field_name"
        examples:
          - "raw.price"
          - "raw.count"
      dimension_inclusion:
        rule: "include string/bool scalar fields; exclude nested objects/arrays"
        examples:
          include: ["status", "type", "category"]
          exclude: ["items", "details", "payload"]
      record_id:
        rule: "metric.id generated by pipeline (ctx.metric_id) OR deterministically hash(source_id + raw_ref.sha256 + metric_name + ts)"
        notes:
          - "Prefer pipeline-generated ULIDs; hashing is fallback and must be stable."

  entity_inference:
    enabled: false
    description: >
      Base profile disables entity inference to avoid accidental PII/PHI expansion.
      Domain overlays may enable with explicit rules and compliance notes.
    requirements_if_enabled:
      - "Entity attributes must not include raw payload blobs."
      - "Entity IDs must be stable and deterministic."
      - "PII/PHI classification must be 'none' or explicitly handled with policy gates."
      - "All inferred entities must validate against contracts."

llm_assist:
  enabled: true
  allowed_roles: ["mapping_suggester"]
  approval_gate: "human_required"
  required_output_format:
    # LLM output must be a patch suggestion only, never an auto-applied change.
    proposed_changes:
      file: "profiles/core/base/mappings.yaml"
      type: "yaml_patch"
      rules:
        - "Only add or modify entries under mapping_rules.*"
        - "Do not change determinism or contract enforcement sections"
        - "Do not enable entity_inference in base profile"
  audit_fields_required:
    - "llm_role"
    - "model_id"
    - "prompt_hash"
    - "output_hash"
    - "approver"
    - "decision"
  notes:
    - "LLM suggestions must be validated by schema + human review."
    - "LLM must not introduce non-deterministic behavior or hidden side effects."

# -----------------------------------------------------------------------------
# EXAMPLES (comments only)
#
# Example A: Map array of objects into metrics
# mapping_rules:
#   domain_example_metrics:
#     enabled: true
#     when: "payload is array"
#     for_each: "payload[*]"
#     emit:
#       - metric_name: "orders.total"
#         ts: "parse_datetime(get(item.created_at, ctx.captured_at))"
#         value: "to_number(item.total)"
#         dimensions:
#           currency: "get(item.currency, 'USD')"
#
# Example B: Webhook payload -> canonical event
# mapping_rules:
#   webhook_event:
#     enabled: true
#     when: "payload.event_name != null"
#     emit:
#       - event_type: "domain.webhook.received"
#         ts: "ctx.captured_at"
#         payload:
#           event_name: "payload.event_name"
#           id: "payload.id"
#
# Example C: Nested payload -> metric with dimensions filter
# mapping_rules:
#   nested_metric:
#     enabled: true
#     when: "payload.data != null"
#     emit:
#       - metric_name: "inventory.available"
#         ts: "ctx.captured_at"
#         value: "to_number(payload.data.available)"
#         dimensions:
#           sku: "to_string(payload.data.sku)"
#           location: "to_string(payload.data.location)"
# -----------------------------------------------------------------------------
